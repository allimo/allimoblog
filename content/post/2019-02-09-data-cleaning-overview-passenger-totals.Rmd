---
title: 'Data Cleaning Overview: FAA Annual Passenger Totals'
author: Allison Moberger
date: '2019-02-09'
slug: data-cleaning-passenger-totals
categories: [R, aviation]
tags:
  - R
  - tidyverse
  - FAA data
  - aviation
  - data import
  - data cleaning
  - workflow
---

```{r options, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, root.dir = here::here())
options(tibble.print_min = 5, tibble.print_max = 10, tibble.width = 75)
```


# Preface

These days I'm working at [CARFAX](https://www.carfax.com/), but prior to that, I spent several years supporting the [Federal Aviation Administration](https://www.faa.gov/) (FAA)'s air traffic control systems. For most of that time, I was also taking grad school courses in the evenings to earn a [masters degree in data analytics](https://catalog.gmu.edu/colleges-schools/engineering/data-analytics-engineering-ms/). So naturally, I started trying to apply the analytical methods I was studying to my everyday work. By the time I was leaving the FAA, I had accumulated a list of aviation topics I wanted to explore, but hadn't yet gotten to.

Since then, I've poured a lot of time and energy into learning [R](https://www.r-project.org/about.html) to work with data, and my mind kept circling back to that list of unfinished projects. Finally, at [rstudio::conf](https://www.rstudio.com/conference/) last month, I listened to amazing people talking about the cool work they're doing, and thought, "man, I want to do stuff like that!" So this blog is going to be my first attempt at [public work](https://resources.rstudio.com/rstudio-conf-2019/the-unreasonable-effectiveness-of-public-work), and where better to start than finally exploring those projects from my aviation to-do list! Better late than never, right?

# Background

There is an absolute [ton of data](https://www.faa.gov/air_traffic/by_the_numbers/media/Air_Traffic_by_the_Numbers_2018.pdf) in the aviation universe, being created and archived by the incredibly complex and sophisticated technologies that drive the industry. Much of it is even [available to the public](https://aspm.faa.gov/). It's not the easiest to find or use; formats and terminology change over the years, and government websites are a labyrinth of acronyms. But government does have a knack for archiving information, so we can find a surprising amount of it to play with.

One handy source is the FAA's [annual commercial statistics reports](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/): counts of total passenger boardings and total cargo weight per airport for all the airports in the country. These are used to allocate airport funding for future fiscal years, so annual data usually first becomes available in the summer of the following year, and is finalized when the funding is finalized in September. So the latest data at present is for calendar year (CY) 2017, but we can work with that.

I'm planning to use this data for other analyses, so I've downloaded [all the years that are available](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/previous_years/) on the website, which goes back to CY 2000. While doing so I spotted a couple of links that were out of place: in [2001](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/previous_years/#2001) and [2011](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/previous_years/#2011), they published historical summary data tables, with overall totals for the entire nation back to 1979. 

This seems like a fun place to start with a really basic question: **what does the total volume of US commercial aviation look like over time?** We'll use this as our motivating question to dive into an overview of data import and cleaning.

# Import

To start with, since the data is in Excel files, I'm using the [readxl](https://readxl.tidyverse.org/) package in conjunction with possibly one of the best not-yet-included-with-tidyverse functions in the world, `clean_names` from [janitor](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#clean-data.frame-names-with-clean_names). I use this habitually with literally every dataframe I import, because capital letters are the bane of my R existence. And of course the powerhouse of tidyverse, [dplyr](https://dplyr.tidyverse.org/), because we are definitely going to be using it later, but also because it conveniently re-exports [magrittr](https://magrittr.tidyverse.org/)'s pipe operator `%>%` which makes analysis workflows vastly clearer and more convenient to write and to read. There are a few other packages here that we'll call on further along, too.

```{r libraries}
library(dplyr)
library(here)
library(janitor)
library(purrr)
library(readr)
library(readxl)
library(tidyr)
```

With two source files, I could easily just hardcode the file paths, but I usually try to avoid this. Instead, I'm hardcoding only a relative path to the right data directory from my project root, and letting `here::here` take care of [making it into a path](https://github.com/jennybc/here_here), which ensures that even if I move this whole project's location on my computer, or even put my script files into a subdirectory (which they actually are already, because that's how [blogdown](https://bookdown.org/yihui/blogdown/) rolls), this script will still find the data. 

```{r data_path}
data_path <- "static/data/faa/historical"

data_files <- here(data_path) %>% dir(pattern = "xls", full.names = TRUE)
```

I also used `dir` to get a list of filenames instead of hardcoding them. This isn't as common, but I think this is good practice, because if a file is renamed, deleted, added, etc. your script will then notice the change and not throw errors because of an invalid path. But also, most of my work uses data directories containing tens or hundreds of files with the same format and systematic naming, and starting with `dir` and `set_names` makes for a great pipeline into `purrr::map_df` to read them in and combine them. More on that in a future blog post.

Anyway, we have two Excel files containing three tables, because the 2001 file inexplicably stores two separate tables on the same sheet. (Note: don't do this.) So for 2001's data, we'll have to use the `range` argument of `read_excel` to specify which parts of the sheet we want going into which object:

```{r import_2001}
passengers_01 <- data_files[[1]] %>% 
    read_excel(range = "A8:F31") %>%
    clean_names()
passengers_01

cargo_01 <- data_files[[1]] %>%
    read_excel(range = "A40:D56") %>%
    clean_names()
cargo_01
```

The formatting of the 2001 tables looks fine without any extra cleanup. 2011 is a different story:

```{r import_2011}
raw_11 <- data_files[[2]] %>% 
    read_excel() %>%
    clean_names() 
raw_11
```

# Tidy

The 2011 data is somewhat untidy. The first column combines two columns, and we also have added two completely blank columns on the far right. We can easily drop those by defining a simple predicate function and using it in either `purrr::keep` or `dplyr::select_if`. (They do the same thing here, but I like `keep` because it's shorter and it works on data in lists as well as dataframes.)

```{r drop_blanks}
not_blank <- function(x) {
    any(!is.na(x))
}

kept_11 <- raw_11 %>% keep(not_blank)
kept_11
```

We also need to fix that leftmost column. This is where the [tidyr](https://tidyr.tidyverse.org/) package shines, specifically its `separate` function:

```{r separate}
passengers_11 <- kept_11 %>% 
    separate(1, into = c("cy", "fy"), sep = "/", convert = TRUE)
passengers_11
```

After years of doing this in Excel with silliness like `LEFT($A2, SEARCH("/", $A2) - 1)` and `RIGHT($A2, LEN($A2) - SEARCH("/", $A2))`, I fell in love with tidyr at first sight.

# Consolidate

We now have some tidy-looking tables, but with a lot of extraneous information. Let's just look at passenger counts for now, and make sure they match up in the overlapping years. We'll make it easy by using the ability to rename columns within a `select` call, just like we do with SQL's `SELECT x AS y`:

```{r just_pass}
pass_01 <- passengers_01 %>% select(year, pass_01 = passenger)
pass_11 <- passengers_11 %>% select(year = cy, pass_11 = total_passengers)

pass_match <- pass_01 %>% 
    full_join(pass_11, by = "year") %>% 
    arrange(year) %>% 
    mutate(agree = pass_01 == pass_11)
pass_match
```

I don't see any `FALSE`s in there (just `NA`s where a year was in one table and not in the other), so all the years that appear in both tables match up! That makes it easy to chop this down to a final table using `coalesce`, which gets the first non-NA value from those listed, just like SQL's `COALESCE` does. We'll use it inside a `transmute` call, which works just like `mutate` + `select`, so we just keep the columns named in `transmute`:

```{r coalesce}
passengers <- pass_match %>% 
    transmute(year = year,
              passengers = coalesce(pass_01, pass_11))
passengers
```

Now before we do anything else, we're going to save this simple, tidy dataframe in a new .csv file, so that in the future we can use it directly without all the intermediate steps. And again, the choice at the beginning to hard-code in the data *directory* but not the *file* makes life much easier:

```{r write, eval = FALSE}
cleaned_dir <- here(data_path, "clean")

if (!dir.exists(cleaned_dir)) {
    dir.create(cleaned_dir)
}

cleaned_file_pass <- here(data_path, "clean", "passengers.csv")
passengers %>% write_csv(cleaned_file_pass)

cleaned_file_cargo <- here(data_path, "clean", "cargo.csv")
cargo_01 %>% write_csv(cleaned_file_cargo)
```

# Visualize

Now we can actually try to answer our original question: **what does the US aviation volume look like over time?** The cargo dataset that we currently have is pretty limited (since it only goes through 2001), so let's focus on passengers for now. 

Because these numbers are so large and difficult to compare, we'll provide a secondary axis that recasts the totals as a percent of the earliest year in the dataset (1979), and some annotation rectangles as backgrounds to highlight those benchmarks. (Since the data import & cleaning, not the visualization, is the focus of this post, I won't go into a lot of detail about all the parts of this plot.) 

```{r viz}
library(ggplot2)
library(scales)

pass_79 <- passengers[[1,2]] #extract get the baseline value

passengers %>% ggplot(aes(x = year, y = passengers)) +
    
    #make the background
    theme(panel.background = element_rect(fill = "lightblue1")) +
    annotate("rect", xmin = -Inf, xmax = Inf, alpha = c(0.3, 0.2),
             ymin = c(-Inf, pass_79 * 2), ymax = c(pass_79 * 1, Inf),
             fill = c("limegreen", "deepskyblue1")) +
    geom_hline(yintercept = pass_79 * 1:2, size = 1, 
               color = c("green4", "deepskyblue"), alpha = 0.8) +
    
    #plot the data
    geom_line(color = "grey30") + 
    geom_point(color = "grey30", size = 2, alpha = 0.8) +

    #adjust the scales
    scale_x_continuous(breaks = seq(1980, 2010, 5), minor_breaks = NULL) +
    scale_y_continuous(labels = comma_format(scale = 0.000001, suffix = " M"),
                       sec.axis = sec_axis(trans = ~ (./pass_79), 
                                           name = "percent of 1979 total",
                                           labels = percent,
                                           breaks = seq(1, 2.5, 0.25))) +
    #add the labels
    labs(title = "Commercial aviation passenger volume takes off",
         subtitle = paste("total passengers weathered turbulence,",
                          "now more than double 1980s volume"),
         caption = "historical passenger totals from FAA.gov",
         y = "total passengers (millions)")
```

I like to make my plots colorful, so I'm using colors suggestive of aviation here, and added some newspaper-headline-worthy puns in the titles for flavor.

# Conclusions

Obviously, a lot more people are flying today (8 years ago, at least) than were in the 1980s. Even with the dip in 1980-1981, volume had already doubled by 1996; it then took a big hit in 2001-2004, and another in 2008 that it was still recovering from in 2011. 

This isn't terribly thrilling information - I suspect most of us would have guessed this trend before we started. We'll look at more detailed data like this in future posts, and see what less-obvious trends might be hiding beneath the surface.

But also, now we've worked through a very basic data import and cleaning process. Next we'll build it out to handle larger and more complicated datasets.


